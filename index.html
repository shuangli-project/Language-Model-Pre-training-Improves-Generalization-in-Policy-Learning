

<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());
  		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- Custom styles for this template -->
	<link href="files/jumbotron.css" rel="stylesheet">
</head>


<style>
.center2 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
.center3 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 80%;
}
.center4 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 70%;
}
.center5 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 100%;
}
</style>


<title>vh-language</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
		<a class="navbar-brand" href="#">Pre-Trained Language Models for Interactive Decision-Making</a>

		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
	  		<span class="navbar-toggler-icon"></span>
		</button>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Abstract">Abstract</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Paper">Paper</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Model">Model</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Results">Results</a>
				</li>
			</ul>
		</div>
	</nav>

	<div class="container" style="padding-top: 80px; font-size: 20px">
		<div align="center">
			<h2 class="text-center" align="center">
				Pre-Trained Language Models for Interactive Decision-Making
			</h2><br>

			<a href="https://people.csail.mit.edu/lishuang/">Shuang Li<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a href="https://people.csail.mit.edu/xavierpuig/">Xavier Puig<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a href="https://yilundu.github.io/">Yilun Du<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a href="https://clintonjwang.github.io/">Clinton Wang<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a href="https://www.ekinakyurek.me/">Ekin Akyürek<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp; <br>
			<a href="http://web.mit.edu/torralba/www/">Antonio Torralba<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a href="https://www.mit.edu/~jda/">Jacob Andreas<sup>1</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en">Igor Mordatch<sup>2</sup></a> 
			<br>
			<!-- <br> -->
			<a><sup>1</sup>MIT CSAIL</a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a><sup>2</sup>Google Brain</a><br>
			<br>
			<a href="https://people.csail.mit.edu/lishuang/"><b>Paper</b></a> &nbsp;&nbsp;&nbsp;&nbsp;
			<a href="https://github.com/ShuangLI59/Language-Model-Pre-training-Improves-Generalization-in-Policy-Learning"><b>Github</b></a><br>

			<!-- <a href="https://www.csail.mit.edu/"><b><sup>1</sup>MIT CSAIL</b></a><br> -->
			<!-- <small>(* indicate equal contribution)</small> -->

		</div>
	</div><br>


	<br>
	<!-- <br> -->
	<!-- <br><br> -->
	

	

	<div class="container">
		<hr class="my-4">
		<h3 id="tem1" style="padding-top: 80px; margin-top: -80px;">Policy generated by a pre-trained language model for a given household task</h3>
		<!-- <img class="img-responsive img-rounded" src="imgs_new/teaser1.gif" style="width:100%; height:100%" alt=""> -->
		<br>
		<video width="800" controls autoplay loop class=center5>
			<source src="imgs_new/teaser2.mp4" type="video/mp4">
		</video>
		<br>
		The policy learned by fine-tuning the pre-trained language model successfully finishes the task described in the goal predicates. 
		We highlight the key actions in the map, where the agent is finding, grabbing, or placing objects in the target positions.
		<br><br>
	</div>


	<br><br>


	<!-- Abstract -->
	<div class="container">
		<hr class="my-4">
		<h3 id="Abstract" style="padding-top: 80px; margin-top: -80px;">Abstract</h3>
		Language model (LM) pre-training has proven useful for a wide variety of language processing tasks, but can such pre-training be leveraged for more general machine learning problems? We investigate the effectiveness of language modeling to scaffold learning and generalization in autonomous decision-making. We describe a framework for imitation learning in which goals and observations are represented as a sequence of embeddings, and translated into actions using a policy network initialized with a pre-trained transformer LM. We demonstrate that this framework enables effective combinatorial generalization across different environments, such as VirtualHome and BabyAI. In particular, for test tasks involving novel goals or novel scenes, initializing policies with language models improves task completion rates by 43.6% in VirtualHome. We hypothesize and investigate three possible factors underlying the effectiveness of LM-based policy initialization. We find that sequential representations (vs. fixed-dimensional feature vectors) and the LM objective (not just the transformer architecture) are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.
	</div>
	<br><br>
	





	
	<div class="container">
		<hr class="my-4">
		<h3 id="Paper" style="padding-top: 80px; margin-top: -80px;">Paper</h3>

		<div class="row">
			<div class="col-md-12">
			<a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a>,
			<a href="https://people.csail.mit.edu/xavierpuig/">Xavier Puig</a>,
			<a href="https://yilundu.github.io/">Yilun Du</a>,
			<a href="https://clintonjwang.github.io/">Clinton Wang</a>,
			<a href="https://www.ekinakyurek.me/">Ekin Akyürek</a>,
			<a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a>,
			<a href="https://www.mit.edu/~jda/">Jacob Andreas</a>, and
			<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en">Igor Mordatch</a><br>

			<b>Pre-Trained Language Models for Interactive Decision-Making</b><br>

			<b>arxiv 2022</b> 
			<a href="">[Paper]</a>
			<a href="https://github.com/ShuangLI59/Pre-Trained-Language-Models-for-Interactive-Decision-Making">[Code]</a>
			<a href="paper.bib" target="_blank">[BibTex]</a><br>

			</div>
		</div>
	</div>
	<br>
	<!-- <br> -->


	<div class="container">
		<img class="img-responsive img-rounded" src="imgs_new/qualitative.png" style="width:100%; height:100%" alt="">
		
		We show two examples of successes in VirtualHome, two examples of successes in BabyAI, and two failure cases caused by the grounding error and policy error. We only show a sub-trajectory in each example and omit most exploration actions to save space. The interacted objects are labelled by green bounding boxes.
	</div>

	<br>


	<div class="container">
		<hr class="my-4">
		<h3 id="Model" style="padding-top: 80px; margin-top: -80px;">Can pre-trained language models be used as a general framework for tasks across different environment?</h3>
		<br>

		In this paper, we study this question through the lens of <b>embodied decision-making</b>, investigating the effectiveness of LM pre-training as a general framework for learning policies across a variety of environments.

		<br><br>
		We propose to use <b>pre-trained language models as a general framework</b> for interactive decision-making across a variety of environments by converting all policy inputs into sequential data.
		
		<br><br>
		This framework is generic, accommodating goals and environment states represented as natural language strings, image patches, or scene graphs.

		<br><br>

		<img class="img-responsive img-rounded" src="imgs_new/1_framework3.png" style="width:100%; height:100%" alt="">

		<br><br>




		<hr class="my-4">
		<h3 id="Results" style="padding-top: 80px; margin-top: -80px;">Combinatorial generalization to out-of-distribution tasks</h3>

		We find that using pre-trained LMs as policy initializers improves in-domain performance and enables several forms of strong generalization over tasks.
		For i.i.d. training and evaluation tasks, we find that this approach yields 20% more successful policies than other baseline methods in VirtualHome.

		<br><br>

		For combinatorial generalization to out-of-distribution tasks, i.e. tasks involving new combinations of goals, states or objects, we find that LM pre-training confers even more benefits: it improves task completion rates by 43.6% for tasks involving novel goals.

		<br><br>

		<div class="center2">
			<img class="img-responsive img-rounded" src="imgs_new/1baselines.png" style="width:90%; height:100%" alt="">
		</div>

		<!-- <br><br> -->
		




		<hr class="my-4">
		<h3 style="padding-top: 80px; margin-top: -80px;">Is the effective combinatorial generalization because LMs are effective models of relations between natural language descriptions of states and actions, or because they provide a more general framework for combinatorial generalization in decision-making?</h3>

		We hypothesize and investigate three possible factors underlying the effectiveness of language modeling for generalization in policy learning:
		(1) input encoding scheme;
		(2) sequential input representations;
		and (3) parameter pre-training.

		<br><br>

		
	
		<h4 style="padding-top: 80px; margin-top: -80px;">(1) Input encoding scheme</h4>
		We investigate (1) by encoding the environment as different types of sequences. Different input encoding schemes have only a negligible impact on model performance: the effectiveness of language modeling is not limited to utilizing natural strings, but in fact extends to arbitrary sequential encodings.

		<br><br>
		<div class="center2">
			<img class="img-responsive img-rounded" src="imgs_new/table1.png" style="width:80%; height:100%" alt="">
		</div>

		Success rates of policies trained with different input encodings in the Novel Tasks setting of VirtualHome. The text encoding is most sample-efficient, but all models converge to similar performance given sufficient training data.


		<br><br>

		<div class="center2">
			<img class="img-responsive img-rounded" src="imgs_new/table2.png" style="width:80%; height:100%" alt="">
		</div>
		Success rate of policies trained with text encoding vs. convolutional encoding in BabyAI. The text encoding is more sample-efficient, but both models converge to near perfect performance given sufficient training data.

		
		<br><br>

		<h4 style="padding-top: 80px; margin-top: -80px;">(2) Sequential input representations</h4>
		We investigate (2) by encoding observations with a single vector embedding, thereby removing its sequential structure (No-Seq). This operation significantly hurts the model's performance on novel tasks.

		
		<br><br>

		<h4 style="padding-top: 80px; margin-top: -80px;">(3) Parameter pre-training</h4>
		Finally, we investigate (3) by learning the parameters of the policy network from scratch (No-Pretrain). The success rate on novel tasks after removing the pre-trained LM weights drops by 11.2%.

		<br>
		<div class="center2">
			<img class="img-responsive img-rounded" src="imgs_new/2seqinput.png" style="width:80%; height:100%" alt="">
		</div>
		"ML-Text (Ours)"" refines a pre-trained LM while "No-Pretrain" learns it from scratch. "No-FineTune" freezes the pre-trained weights. "No-Seq" uses non-sequential inputs. Fine-tuning the pre-trained weights and the usage of sequential encoding are important for combinatorial generalization.



		<br><br>
		

		We find that sequential representations (vs. fixed-dimensional feature vectors) and the LM objective (not just the transformer architecture) are both important for generalization, however, the input encoding schemes (e.g. as a natural language string vs. an arbitrary encoding scheme) has little influence.

	</div>





	<!-- <div class="container">
		<hr class="my-4">
		<h3 id="tem1" style="padding-top: 80px; margin-top: -80px;">Policy generated by a pre-trained language model for a given household task</h3>
		The policy learned by fine-tuning the pre-trained language model successfully finishes the task described in the goal predicates. 
		We highlight the key actions in the map, where the agent is finding, grabbing, or placing objects in the target positions.
		<br><br><br>
		<img class="img-responsive img-rounded" src="imgs_new/teaser1.gif" style="width:100%; height:100%" alt="">
	</div>


	<br><br> -->


	<!-- Video -->
	<div class="container">
		<hr class="my-4">
		<h3 style="padding-top: 80px; margin-top: -80px;">More Qualitative Results</h3><br>
		<!-- <br> -->


		
		<div align="left">
			<h4 style="padding-top: 80px; margin-top: -80px;">1. Policy with pre-trained Language Model v.s. Policy without pre-trained Language Model</h4>
		  <!-- <div class="embed-responsive embed-responsive-16by9"> -->
		    <!-- <iframe style="border: 1px solid #000;" class="embed-responsive-item" src="imgs_new/vid1.mp4" frameborder="0" allowfullscreen></iframe> -->
		  <!-- </div> -->
		  	<video width="800" controls autoplay loop class=center4>
			  <source src="imgs_new/vid1.mp4" type="video/mp4">
			</video>
		</div>
		<br>


		<div align="left">
			<h4 style="padding-top: 80px; margin-top: -80px;">2. Policy with pre-trained Language Model v.s. Policy with LSTM</h4>
		  	<video width="800" controls autoplay loop class=center4>
			  <source src="imgs_new/vid2.mp4" type="video/mp4">
			</video>
		</div>
		<br>

		<div align="left">
			<h4 style="padding-top: 80px; margin-top: -80px;">3. Policy with pre-trained Language Model on different test settings</h4>
		  	<video width="800" controls autoplay loop class=center4>
			  <source src="imgs_new/vid3.mp4" type="video/mp4">
			</video>
		</div>
		<br>


	</div>
	<br><br><br><br>



<!-- 
  	<div class="container">
		<h3 id="Model" style="padding-top: 80px; margin-top: -80px;">Model Overview</h3>
		<br>

		<h4 style="padding-top: 80px; margin-top: -80px;">1. Overview of VirtualHome</h4>
		<br>
	  	<img class="img-responsive img-rounded" src="imgs/environment.png" style="width:100%; height:100%" alt="">
		<br><br><br>

		<h4 style="padding-top: 80px; margin-top: -80px;">2. Overview of the training procedure</h4>
		<br>
	  	<img class="img-responsive img-rounded" src="imgs/model3.png" style="width:100%; height:100%" alt="">
		<br><br><br>
	</div>
	<br><br><br><br>
 -->


	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; Massachusetts Institute of Technology 2022</p>
			</footer>
		</center>
	</div>


	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>

<

